# monitor/utils/pdf_processor.py
import os
import re
import logging
import traceback
from io import StringIO
from typing import Tuple, List, Dict, Optional
from datetime import datetime
from transformers import pipeline
import PyPDF2
from pdfminer.high_level import extract_text_to_fp
from pdfminer.layout import LAParams
import nltk
from nltk.tokenize import sent_tokenize
from django.conf import settings
from django.core.exceptions import SuspiciousFileOperation
from django.db import transaction
from monitor.models import Documento, NormaVigente

logger = logging.getLogger(__name__)

class PDFProcessor:
    def __init__(self):
        self._setup_nltk()
        self._setup_patterns()
        self.limite_relevancia = 4
        self.max_retries = 3
        self.timeout = 30

    def _setup_nltk(self):
        """Configura recursos do NLTK com fallback"""
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('tokenizers/punkt_tab/portuguese')
        except LookupError:
            try:
                nltk.download('punkt')
                nltk.download('punkt_tab')
                nltk.download('stopwords')
            except Exception as e:
                logger.warning(f"Falha ao baixar recursos NLTK: {str(e)}")

    def _setup_patterns(self):
        """Configura padr√µes de busca com pesos"""
        self.padroes_contabeis = [
            # Impostos e obriga√ß√µes (alto peso)
            (r'\b(icms|ipi|iss|pis|cofins|csll|irpj)\b', 3),
            (r'\b(sped|efd|dctf|dirf|DAS|darf)\b', 3),
            # Termos gerais (m√©dio peso)
            (r'tribut[a√°]r[i√≠][ao]|fiscaliza[c√ß][a√£]o', 2),
            (r'receita\s+federal|sefaz|fazenda\s+nacional', 2),
            # Conceitos cont√°beis (baixo peso)
            (r'balan[c√ß]o\s+patrimonial|demonstra[c√ß][o√µ]es\s+financeiras', 1)
        ]

        self.padroes_normas = [
            r'(?i)(Lei|Decreto|Portarias da SEFAZ |Instru√ß√£o NormativaConv√™nios e Protocolos do CONFAZ|Resolu√ß√£o)\s+(?:n?[¬∫¬∞]?\s*)?(\d+[\.\/-]?\d*)',
            r'(?i)(Lei Complementar)\s+(n?[¬∫¬∞]?\s*)?(\d+)',
            r'(?i)(Medida Provis√≥ria)\s+(n?[¬∫¬∞]?\s*)?(\d+)'
        ]

    @transaction.atomic
    def processar_documento(self, documento: Documento) -> bool:
        """Processa um documento com tratamento robusto de erros"""
        logger.info(f"Iniciando processamento do documento ID {documento.id}")
        
        try:
            if not self._validar_documento(documento):
                return False

            texto = self._extrair_texto_com_fallback(documento.arquivo_pdf.path)
            if not texto:
                logger.error(f"Falha ao extrair texto do documento ID {documento.id}")
                return False

            relevante, detalhes = self.analisar_relevancia(texto)
            if not relevante:
                logger.info(f"Documento ID {documento.id} n√£o √© relevante")
                self._handle_documento_nao_relevante(documento)
                return False

            return self._processar_documento_relevante(documento, texto, detalhes)

        except Exception as e:
            logger.error(f"Erro cr√≠tico ao processar documento ID {documento.id}: {str(e)}\n{traceback.format_exc()}")
            return False

    def _validar_documento(self, documento: Documento) -> bool:
        """Valida o documento antes do processamento"""
        if not documento.arquivo_pdf:
            logger.warning("Documento sem arquivo PDF associado")
            return False

        try:
            if not os.path.exists(documento.arquivo_pdf.path):
                logger.warning(f"Arquivo PDF n√£o encontrado: {documento.arquivo_pdf.path}")
                return False
                
            if os.path.getsize(documento.arquivo_pdf.path) == 0:
                logger.warning("Arquivo PDF vazio")
                return False
                
            return True
            
        except (SuspiciousFileOperation, OSError) as e:
            logger.error(f"Erro ao validar arquivo: {str(e)}")
            return False

    def _extrair_com_pypdf2(self, caminho_pdf: str) -> str:
        """Extrai texto usando PyPDF2"""
        texto = ""
        with open(caminho_pdf, 'rb') as f:
            leitor = PyPDF2.PdfReader(f)
            for pagina in leitor.pages:
                texto += pagina.extract_text() or ""
        return self._limpar_texto(texto)

    def _extrair_com_pdfminer(self, caminho_pdf: str) -> str:
        """Extrai texto usando pdfminer.six"""
        try:
            output = StringIO()
            with open(caminho_pdf, 'rb') as f:
                extract_text_to_fp(
                    f,
                    output,
                    laparams=LAParams(),
                    output_type='text',
                    codec='utf-8'
                )
            return self._limpar_texto(output.getvalue())
        except Exception as e:
            logger.warning(f"Falha no pdfminer: {str(e)}")
            return None

    def _extrair_com_fallback_alternativo(self, caminho_pdf: str) -> str:
        """M√©todo alternativo para extra√ß√£o de texto - vers√£o simplificada"""
        try:
            # Tenta ler como texto puro (para PDFs que s√£o na verdade arquivos de texto)
            with open(caminho_pdf, 'r', encoding='utf-8', errors='ignore') as f:
                texto = f.read(5000)  # L√™ apenas os primeiros 5KB para verifica√ß√£o
                if "PDF" not in texto[:20]:  # Se n√£o parece ser um PDF bin√°rio
                    return self._limpar_texto(texto)
        except:
            pass
            
        return None

    def _extrair_texto_com_fallback(self, caminho_pdf: str) -> Optional[str]:
        """Extrai texto com m√∫ltiplas estrat√©gias de fallback"""
        strategies = [
            self._extrair_com_pypdf2,
            self._extrair_com_pdfminer,
            self._extrair_com_fallback_alternativo
        ]
        
        for strategy in strategies:
            try:
                texto = strategy(caminho_pdf)
                if texto and len(texto.strip()) > 100:
                    return texto
            except Exception as e:
                logger.warning(f"Falha na estrat√©gia {strategy.__name__}: {str(e)}")
                continue
                
        return None

    def _limpar_texto(self, texto: str) -> str:
        """Normaliza o texto extra√≠do"""
        if not texto:
            return ""
            
        texto = re.sub(r'\s+', ' ', texto)
        texto = re.sub(r'-\n', '', texto)
        texto = re.sub(r'[^\w\s√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√ß√Å√â√ç√ì√ö√Ç√ä√é√î√õ√É√ï√á.,;:!?()\-¬∫¬∞%$]', '', texto)
        return texto.strip()

    def analisar_relevancia(self, texto: str) -> Tuple[bool, Dict]:
        """Analisa a relev√¢ncia cont√°bil com pontua√ß√£o detalhada"""
        if not texto:
            return False, {}
            
        texto = texto.lower()
        detalhes = {
            'pontuacao': 0,
            'termos': {},
            'normas': []
        }

        for padrao, peso in self.padroes_contabeis:
            matches = re.findall(padrao, texto)
            if matches:
                termo = matches[0] if isinstance(matches[0], str) else matches[0][0]
                detalhes['termos'][termo] = detalhes['termos'].get(termo, 0) + len(matches)
                detalhes['pontuacao'] += peso * len(matches)

        normas = self.extrair_normas(texto)
        detalhes['normas'] = normas
        detalhes['pontuacao'] += len(normas) * 2

        relevante = detalhes['pontuacao'] >= self.limite_relevancia
        return relevante, detalhes

    def extrair_normas(self, texto: str) -> List[Tuple[str, str]]:
        """Extrai normas com valida√ß√£o rigorosa"""
        normas = []
        for padrao in self.padroes_normas:
            for match in re.finditer(padrao, texto, re.IGNORECASE):
                try:
                    tipo = self._normalizar_tipo_norma(match.group(1))
                    numero = self._normalizar_numero_norma(match.group(2))
                    if tipo and numero:
                        normas.append((tipo, numero))
                except (IndexError, AttributeError):
                    continue
                    
        seen = set()
        return [n for n in normas if not (n in seen or seen.add(n))]

    def _normalizar_tipo_norma(self, tipo: str) -> str:
        """Normaliza o tipo de norma"""
        if not tipo:
            return ""
            
        tipo = tipo.upper().strip()
        mapeamento = {
            'LEI COMPLEMENTAR': 'LC',
            'MEDIDA PROVIS√ìRIA': 'MP',
            'INSTRU√á√ÉO NORMATIVA': 'IN',
            'RESOLU√á√ÉO': 'RES'
        }
        return mapeamento.get(tipo, tipo.split()[0])

    def _normalizar_numero_norma(self, numero: str) -> str:
        """Normaliza o n√∫mero da norma"""
        if not numero:
            return ""
            
        numero = re.sub(r'[^\d\/]', '', numero)
        return numero.strip()

    def _processar_documento_relevante(self, documento: Documento, texto: str, detalhes: Dict) -> bool:
        """Processa um documento considerado relevante"""
        try:
            documento.texto_completo = texto
            documento.resumo = self._gerar_resumo(texto, detalhes)
            documento.relevante_contabil = True
            documento.processado = True
            
            for tipo, numero in detalhes.get('normas', []):
                norma, _ = NormaVigente.objects.get_or_create(
                    tipo=tipo,
                    numero=numero,
                    defaults={'situacao': 'A VERIFICAR', 'fonte': 'DIARIO_OFICIAL'}
                )
                documento.normas_relacionadas.add(norma)
            
            documento.save()
            logger.info(f"Documento ID {documento.id} processado com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"Falha ao processar documento relevante ID {documento.id}: {str(e)}")
            return False



    def _gerar_resumo(self, texto: str, detalhes: Dict) -> str:
        # Usa BERT para sumariza√ß√£o extrativa (mant√©m frases originais)
        summarizer = pipeline("summarization", model="neuralmind/bert-base-portuguese-cased")
        resumo = summarizer(texto, max_length=150, min_length=30, do_sample=False)
        
        # Combina com informa√ß√µes de normas/termos
        partes = []
        if detalhes.get('normas'):
            normas_str = ", ".join(f"{tipo} {numero}" for tipo, numero in detalhes['normas'][:3])
            partes.append(f"üîπ **Normas mencionadas**: {normas_str}")
        
        partes.append(f"üìå **Resumo autom√°tico**: {resumo[0]['summary_text']}")
        return "\n\n".join(partes)

    def _extrair_trechos_relevantes(self, texto: str) -> List[str]:
        """Extrai trechos relevantes do texto"""
        sentences = sent_tokenize(texto, language='portuguese')
        relevantes = []
        
        for sent in sentences:
            if len(relevantes) >= 3:
                break
                
            for padrao, _ in self.padroes_contabeis:
                if re.search(padrao, sent, re.IGNORECASE):
                    relevantes.append(sent.strip())
                    break
                    
        return relevantes

    def _handle_documento_nao_relevante(self, documento: Documento):
        """Lida com documentos n√£o relevantes conforme configura√ß√£o"""
        try:
            if getattr(settings, 'REMOVER_NAO_RELEVANTES', False):
                documento.delete()
                logger.info(f"Documento ID {documento.id} removido por irrelev√¢ncia")
            else:
                documento.arquivo_pdf.delete(save=False)
                documento.arquivo_removido = True
                documento.save()
                logger.info(f"Arquivo do documento ID {documento.id} removido")
        except Exception as e:
            logger.error(f"Falha ao lidar com documento n√£o relevante ID {documento.id}: {str(e)}")

    def processar_todos_documentos(self) -> Dict[str, int]:
        """Processa todos os documentos n√£o processados"""
        docs = Documento.objects.filter(processado=False)
        logger.info(f"Iniciando processamento em lote de {docs.count()} documentos")
        
        resultados = {
            'total': docs.count(),
            'sucesso': 0,
            'irrelevantes': 0,
            'falhas': 0
        }
        
        for doc in docs:
            try:
                if self.processar_documento(doc):
                    resultados['sucesso'] += 1
                else:
                    resultados['irrelevantes'] += 1
            except Exception as e:
                resultados['falhas'] += 1
                logger.error(f"Falha no documento ID {doc.id}: {str(e)}")
        
        logger.info(
            f"Processamento conclu√≠do: "
            f"{resultados['sucesso']} sucessos, "
            f"{resultados['irrelevantes']} irrelevantes, "
            f"{resultados['falhas']} falhas"
        )
        return resultados